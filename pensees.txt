Is putting "music" into core concept for the first iteration a good idea?
    - It extracts many names and genres
    - Good for Ontology, but might bias the machine learning part


Used word2vec model to help filter NPs. We got 3416568 NPs (first try), which is way too much.
Taking a closer look, the NP extraction is not perfect, so it's worth filtering with common stop words, though
this may impact some real hypernymy couples using this stop words.

After filtering with stop words, we got 2092611 NPs


Train of the without_number model:
 effective_min_count=40 retains 50838 unique words (4% of original 1137022, drops 1086184)
2021-01-29 02:06:53,202 : INFO : effective_min_count=40 leaves 86081554 word corpus (96% of original 89239873, drops 3158319)
2021-01-29 02:06:53,316 : INFO : deleting the raw counts dictionary of 1137022 items
2021-01-29 02:06:53,336 : INFO : sample=0.001 downsamples 49 most-common words
2021-01-29 02:06:53,336 : INFO : downsampling leaves estimated 67515656 word corpus (78.4% of prior 86081554)
estimated required memory for 50838 words and 300 dimensions: 147430200 bytes
2021-01-29 02:06:53,439 : INFO : resetting layer weights
2021-01-29 02:07:00,780 : INFO : training model with 4 workers on 50838 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10



After filtering with single number appearance, we got 1992488 NPs

Forth try of filtering NPs:
Added more stop words, and filter NP that is longer than 3 words, and we got 1242839 NPs

removed influence of first capital letter in a sentence and we got : 1055942 NPs